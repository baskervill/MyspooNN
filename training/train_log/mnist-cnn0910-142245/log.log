[32m[0910 14:22:45 @logger.py:74][0m Argv: mnist-cnn.py 1
[32m[0910 14:22:45 @fs.py:89][0m [5m[31mWRN[0m Env var $TENSORPACK_DATASET not set, using /home/baskerville/tensorpack_data for datasets.
[32m[0910 14:22:46 @inference_runner.py:81][0m InferenceRunner will eval 40 iterations
[32m[0910 14:22:46 @base.py:324][0m [5m[31mWRN[0m You're calling new trainers with old trainer API!
[32m[0910 14:22:46 @base.py:325][0m [5m[31mWRN[0m Now it returns the old trainer for you, please switch to use new trainers soon!
[32m[0910 14:22:46 @base.py:326][0m [5m[31mWRN[0m See https://github.com/ppwwyyxx/tensorpack/issues/458 for more information.
[32m[0910 14:22:46 @simple.py:37][0m [5m[31mWRN[0m FeedInput is slow (and this is the default of SimpleTrainer). Consider QueueInput or other InputSource instead.
[32m[0910 14:22:46 @registry.py:136][0m conv0 input: [None, 28, 28, 1]
[32m[0910 14:22:46 @registry.py:144][0m conv0 output: [None, 28, 28, 32]
[32m[0910 14:22:46 @registry.py:136][0m pool0 input: [None, 28, 28, 32]
[32m[0910 14:22:46 @registry.py:144][0m pool0 output: [None, 14, 14, 32]
[32m[0910 14:22:46 @registry.py:136][0m conv1 input: [None, 14, 14, 32]
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv1/W
[32m[0910 14:22:46 @registry.py:144][0m conv1 output: [None, 14, 14, 32]
[32m[0910 14:22:46 @registry.py:136][0m conv2 input: [None, 14, 14, 32]
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv2/W
[32m[0910 14:22:46 @registry.py:144][0m conv2 output: [None, 14, 14, 32]
[32m[0910 14:22:46 @registry.py:136][0m pool1 input: [None, 14, 14, 32]
[32m[0910 14:22:46 @registry.py:144][0m pool1 output: [None, 7, 7, 32]
[32m[0910 14:22:46 @registry.py:136][0m conv3 input: [None, 7, 7, 32]
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv3/W
[32m[0910 14:22:46 @registry.py:144][0m conv3 output: [None, 7, 7, 32]
[32m[0910 14:22:46 @registry.py:136][0m fc0 input: [None, 7, 7, 32]
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight fc0/W
[32m[0910 14:22:46 @registry.py:144][0m fc0 output: [None, 20]
[32m[0910 14:22:46 @registry.py:136][0m fc1 input: [None, 20]
[32m[0910 14:22:46 @registry.py:144][0m fc1 output: [None, 10]
[32m[0910 14:22:46 @develop.py:85][0m [5m[31mWRN[0m [Deprecated] accuracy [mnist-cnn.py:284] will be deprecated after 28 Feb. Please implement it by yourself.
[32m[0910 14:22:46 @regularize.py:81][0m regularize_cost() found 2 tensors.
[32m[0910 14:22:46 @regularize.py:18][0m Applying regularizer for fc0/W:0, fc1/W:0
[32m[0910 14:22:46 @model_utils.py:49][0m [36mModel Parameters: 
[0mname         shape             dim
-----------  --------------  -----
conv0/W:0    [3, 3, 1, 32]     288
bn0/beta:0   [32]               32
bn0/gamma:0  [32]               32
conv1/W:0    [3, 3, 32, 32]   9216
bn1/beta:0   [32]               32
bn1/gamma:0  [32]               32
conv2/W:0    [3, 3, 32, 32]   9216
bn2/beta:0   [32]               32
bn2/gamma:0  [32]               32
conv3/W:0    [3, 3, 32, 32]   9216
bn3/beta:0   [32]               32
bn3/gamma:0  [32]               32
fc0/W:0      [1568, 20]      31360
fc1/W:0      [20, 10]          200[36m
Total #vars=14, #params=59752, size=0.23MB[0m
[32m[0910 14:22:46 @base.py:141][0m Setup callbacks graph ...
[32m[0910 14:22:46 @predict.py:42][0m Building predictor tower 'InferenceTower' on device /gpu:0 ...
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv1/W
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv2/W
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight conv3/W
[32m[0910 14:22:46 @mnist-cnn.py:73][0m Quantizing weight fc0/W
[32m[0910 14:22:46 @develop.py:85][0m [5m[31mWRN[0m [Deprecated] accuracy [mnist-cnn.py:284] will be deprecated after 28 Feb. Please implement it by yourself.
[32m[0910 14:22:46 @summary.py:34][0m Maintain moving average summary of 5 tensors.
[32m[0910 14:22:46 @graph.py:90][0m Applying collection UPDATE_OPS of 8 ops.
[32m[0910 14:22:46 @base.py:146][0m Creating the session ...
[32m[0910 14:22:47 @base.py:150][0m Initializing the session ...
[32m[0910 14:22:47 @base.py:157][0m Graph Finalized.
[32m[0910 14:22:47 @base.py:191][0m Start Epoch 1 ...
[32m[0910 14:22:52 @base.py:201][0m Epoch 1 (global_step 468) finished, time:5.08 sec.
[32m[0910 14:22:52 @saver.py:84][0m Model saved to train_log/mnist-cnn0910-142245/model-468.
[32m[0910 14:22:52 @monitor.py:363][0m accuracy: 0.96943
[32m[0910 14:22:52 @monitor.py:363][0m cross_entropy_loss: 0.11409
[32m[0910 14:22:52 @monitor.py:363][0m learning_rate: 0.1
[32m[0910 14:22:52 @monitor.py:363][0m regularize_loss: 0.0004674
[32m[0910 14:22:52 @monitor.py:363][0m total_cost: 0.11456
[32m[0910 14:22:52 @monitor.py:363][0m train_error: 0.030568
[32m[0910 14:22:52 @monitor.py:363][0m validation_accuracy: 0.96602
[32m[0910 14:22:52 @monitor.py:363][0m validation_cross_entropy_loss: 0.10754
[32m[0910 14:22:52 @monitor.py:363][0m validation_error: 0.0348
[32m[0910 14:22:52 @base.py:191][0m Start Epoch 2 ...
[32m[0910 14:22:57 @base.py:201][0m Epoch 2 (global_step 936) finished, time:4.18 sec.
[32m[0910 14:22:57 @saver.py:84][0m Model saved to train_log/mnist-cnn0910-142245/model-936.
[32m[0910 14:22:57 @saver.py:155][0m Model with maximum 'validation_accuracy' saved.
[32m[0910 14:22:57 @monitor.py:363][0m accuracy: 0.97458
[32m[0910 14:22:57 @monitor.py:363][0m cross_entropy_loss: 0.079289
[32m[0910 14:22:57 @monitor.py:363][0m learning_rate: 0.1
[32m[0910 14:22:57 @monitor.py:363][0m regularize_loss: 0.00047079
[32m[0910 14:22:57 @monitor.py:363][0m total_cost: 0.07976
[32m[0910 14:22:57 @monitor.py:363][0m train_error: 0.025424
[32m[0910 14:22:57 @monitor.py:363][0m validation_accuracy: 0.97637
[32m[0910 14:22:57 @monitor.py:363][0m validation_cross_entropy_loss: 0.078014
[32m[0910 14:22:57 @monitor.py:363][0m validation_error: 0.0242
[32m[0910 14:22:57 @base.py:191][0m Start Epoch 3 ...
[32m[0910 14:23:01 @base.py:201][0m Epoch 3 (global_step 1404) finished, time:4.23 sec.
[32m[0910 14:23:01 @saver.py:84][0m Model saved to train_log/mnist-cnn0910-142245/model-1404.
[32m[0910 14:23:01 @saver.py:155][0m Model with maximum 'validation_accuracy' saved.
[32m[0910 14:23:02 @monitor.py:363][0m accuracy: 0.97986
[32m[0910 14:23:02 @monitor.py:363][0m cross_entropy_loss: 0.064041
[32m[0910 14:23:02 @monitor.py:363][0m learning_rate: 0.1
[32m[0910 14:23:02 @monitor.py:363][0m regularize_loss: 0.0004782
[32m[0910 14:23:02 @monitor.py:363][0m total_cost: 0.064519
[32m[0910 14:23:02 @monitor.py:363][0m train_error: 0.020137
[32m[0910 14:23:02 @monitor.py:363][0m validation_accuracy: 0.98457
[32m[0910 14:23:02 @monitor.py:363][0m validation_cross_entropy_loss: 0.054857
[32m[0910 14:23:02 @monitor.py:363][0m validation_error: 0.0143
[32m[0910 14:23:02 @base.py:191][0m Start Epoch 4 ...
[32m[0910 14:23:04 @base.py:209][0m Detected Ctrl-C and exiting main loop.
